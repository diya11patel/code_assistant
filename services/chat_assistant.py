import io
import re
import shutil
import subprocess
import zipfile
import os
from pathlib import Path
from typing import Any, Dict, List
from fastapi import HTTPException, UploadFile

from db_connection.qdrant import QdrantDBManager, COLLECTION_NAME 
from dto.value_objects import ChunkResponse, CodeChunk, QueryResponse, UserQueryAnalysisType, LLMQueryResponse, CodeReference
from langugae_processors.php_processor import LaravelProcessor 
from model_interfaces.embedding_model import EmbeddingModel
from model_interfaces.gemini_model import GeminiModel
from model_interfaces.prompts import gemini_prompts
from utils.logger import LOGGER
from utils.utility import get_file_type, normalize_line,normalize_lines

import patch
from unidiff import Hunk, PatchSet
from typing import List, Dict, Any
from difflib import SequenceMatcher
from transformers import GPT2TokenizerFast

class ChatAssistantService():
    def __init__(self):
        self.embedding_model = EmbeddingModel()
        self.vector_store = QdrantDBManager()
        try:
            self.llm_model = GeminiModel()
        except Exception as e: 
            LOGGER.info(f"Failed to initialize Gemini Model: {e}. LLM features will be unavailable.")
            self.llm_model = None
        self.php_processor = LaravelProcessor()  # Initialize once to reuse

    async def process_uploaded_zip(self, zip_file: UploadFile, description: str) -> list[str]:
        """
        Handles the end-to-end process of receiving a zip file,
        saving, extracting, and initiating processing (parsing, embedding, storage).

        Args:
            zip_file (UploadFile): The uploaded zip file.
            description (str): Description of the codebase.

        Returns:
            list[str]: A list of file paths that were extracted.

        """
        LOGGER.info("Started codebase Processing")
        # project_path = r"D:/codes/langGraph_venv/code_assistant/temp_code_uploads/leave-management-laravel"
        project_path =  self.save_uploaded_zip(zip_file, description)
        result = await self.process_codebase(project_path, description)
        return True

    async def process_codebase(self, project_path: str, description: str) -> List[ChunkResponse]:
        self.php_processor = LaravelProcessor()
        code_chunks: List[CodeChunk] = self.php_processor.analyze_codebase(project_path)
        
        if not code_chunks:
            LOGGER.info("No code chunks were generated by the processor.")
            return []

        contents_to_embed = [chunk.content for chunk in code_chunks]
        embeddings = self.embedding_model.embed_chunks(contents_to_embed)

        if not embeddings or len(embeddings) != len(code_chunks):
            LOGGER.info(f"Embedding failed or mismatch. Expected {len(code_chunks)} embeddings, got {len(embeddings) if embeddings else 0}.")
            return []

        payloads = [chunk.model_dump() for chunk in code_chunks]
        success = self.vector_store.save_embeddings(embeddings, payloads)

        if not success:
            LOGGER.info("Failed to save embeddings to vector store.")
            return []
        return code_chunks

    def save_uploaded_zip(self, file: UploadFile, description: str) -> str:
        TEMP_DIR = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "temp_code_uploads")
        if not os.path.exists(TEMP_DIR):
            os.makedirs(TEMP_DIR, exist_ok=True)
            LOGGER.info(f"Warning: TEMP_DIR '{TEMP_DIR}' was created by api_router.py. It should ideally be managed by main.py startup.")

        extracted_file_paths = []
        temp_zip_path = os.path.join(TEMP_DIR, file.filename)
        extraction_base_name = os.path.splitext(file.filename)[0]
        extraction_path = os.path.join(TEMP_DIR, extraction_base_name)
        try:
            with open(temp_zip_path, "wb") as buffer:
                shutil.copyfileobj(file.file, buffer)

            if os.path.exists(extraction_path):
                shutil.rmtree(extraction_path)
            os.makedirs(extraction_path, exist_ok=True)

            with zipfile.ZipFile(temp_zip_path, 'r') as zip_ref:
                zip_ref.extractall(extraction_path)
                for item_name in zip_ref.namelist():
                    full_item_path = os.path.join(extraction_path, item_name)
                    extracted_file_paths.append(full_item_path)

            LOGGER.info(f"Received description for {file.filename}: {description}")
            LOGGER.info(f"Extracted files for {file.filename} to: {extraction_path}")

            os.remove(temp_zip_path)

        except Exception as e:
            if os.path.exists(temp_zip_path):
                os.remove(temp_zip_path)
            if os.path.exists(extraction_path) and not any(os.path.exists(p) for p in extracted_file_paths):
                shutil.rmtree(extraction_path)
            raise HTTPException(status_code=500, detail=f"Failed to process zip file '{file.filename}': {str(e)}")
        finally:
            if hasattr(file, 'file') and file.file:
                file.file.close()

                # Check if the extraction resulted in a single directory
        # This is common if the zip file itself contained a root folder for the project
        items_in_extraction_path = os.listdir(extraction_path)
        if len(items_in_extraction_path) == 1:
            single_item_path = os.path.join(extraction_path, items_in_extraction_path[0])
            if os.path.isdir(single_item_path):
                LOGGER.info(f"Detected single root folder '{items_in_extraction_path[0]}' in extracted content. Using it as project root.")
                return single_item_path # Return the path to the inner directory

        # If not a single directory, or multiple items, assume extraction_path is the project root
        # (e.g., if the zip was created from the contents of the project root directly)
        LOGGER.info(f"Using extraction path '{extraction_path}' as project root.")
        return extraction_path

    def query_for_semantic_search(self, query: str, top_k: int = 5) -> QueryResponse:
        original_user_query = query

        if not original_user_query:
            return QueryResponse(
                question=original_user_query, 
                llm_answer=LLMQueryResponse(explanation="Query was empty.", code_references=[]), 
                retrieved_chunks=[], 
                relevant_chunks_found=0)

        processed_query_for_search = original_user_query

        if self.llm_model:
            LOGGER.info(f"Analyzing query with LLM: '{original_user_query}'")
            try:
                analysis_result = self.llm_model.analyze_query(original_user_query)
                LOGGER.info(f"LLM analysis result: type='{analysis_result.type.value}', response='{analysis_result.response}'")

                if analysis_result.type == UserQueryAnalysisType.GENERAL_ANSWER:
                    LOGGER.info(f"LLM provided direct answer: {analysis_result.response}")
                    return QueryResponse(
                        question=original_user_query,
                        llm_answer=LLMQueryResponse(explanation=analysis_result.response, code_references=[]),
                        retrieved_chunks=[],
                        relevant_chunks_found=0
                    )
                elif analysis_result.type == UserQueryAnalysisType.REFINED_QUERY:
                    if analysis_result.response and analysis_result.response.strip():
                        processed_query_for_search = analysis_result.response
                        LOGGER.info(f"LLM refined query to: '{processed_query_for_search}'")
                    else:
                        LOGGER.info("LLM returned an empty refined query, using original.")
                else:
                    LOGGER.info(f"LLM analysis returned unknown type: '{analysis_result.type.value}'. Proceeding with original query.")

            except Exception as e:
                LOGGER.error(f"Error during LLM query analysis: {e}. Proceeding with original query.")
        else:
            LOGGER.info("LLM model not available for query pre-processing.")

        # --- Semantic Search with processed_query_for_search ---
        LOGGER.info(f"Embedding processed query for search: '{processed_query_for_search}'")
        query_embedding_list = self.embedding_model.embed_chunks([processed_query_for_search])

        if not query_embedding_list:
            LOGGER.info("Could not generate embedding for the query.")
            return QueryResponse(
                question=original_user_query, 
                llm_answer=LLMQueryResponse(explanation="Could not generate embedding for the query.", code_references=[]), 
                relevant_chunks_found=0, 
                retrieved_chunks=[])
        
        query_embedding = query_embedding_list[0]

        LOGGER.info(f"Searching Qdrant for top {top_k} similar chunks based on: '{processed_query_for_search}'")
        similar_chunks_payloads = self.vector_store.search_similar_chunks(embedding=query_embedding, limit=top_k)
        
        retrieved_chunk_responses: List[ChunkResponse] = []
        code_chunks_for_llm: List[Dict[str, str]] = []

        if similar_chunks_payloads:
            for payload in similar_chunks_payloads:
                file_name = payload.get("file_path", "Unknown")
                chunk_content = payload.get("content", "No content available for this chunk.")
                
                retrieved_chunk_responses.append(
                    ChunkResponse(file_name=file_name, chunk=chunk_content)
                )
                code_chunks_for_llm.append({
                    "file_path": file_name,
                    "content": chunk_content,
                    "start_line": payload.get("start_line", 0),
                    "end_line": payload.get("end_line", 0)
                })
        
        final_llm_answer: LLMQueryResponse
        if self.llm_model:
            if code_chunks_for_llm: # If chunks were found
                LOGGER.info(f"Sending original query {original_user_query} and {len(code_chunks_for_llm)} chunks to LLM for final answer...")
                final_llm_answer = self.llm_model.generate_response(user_query=original_user_query, context_chunks=code_chunks_for_llm)
            else: # No relevant chunks found after search
                LOGGER.info(f"No relevant chunks found. Sending original query {original_user_query} to LLM without specific context chunks.")
                final_llm_answer = self.llm_model.generate_response(user_query=original_user_query, context_chunks=[])
        elif not self.llm_model:
            final_llm_answer = LLMQueryResponse(
                explanation="LLM model is not available to generate an answer.",
                code_references=[]
            )
        else:
            final_llm_answer = LLMQueryResponse(
                explanation="Could not generate an answer using the LLM due to an unexpected state.",
                code_references=[]
            )
        

        return QueryResponse(
            question=original_user_query,
            llm_answer=final_llm_answer,
            retrieved_chunks=retrieved_chunk_responses,
            relevant_chunks_found=len(retrieved_chunk_responses)
        )

    #new method for qdrant updation 
    def overwrite_chunk_in_file(self, file_path: str, start_line: int, end_line: int, new_content: str, project_root: str) -> Dict[str, Any]:
        """
        Overwrites a specific chunk in a file between start_line and end_line with new_content.

        Args:
            file_path (str): Relative path to the file (e.g., 'app/Http/Controllers/LeaveController.php').
            start_line (int): Starting line number of the chunk (1-based).
            end_line (int): Ending line number of the chunk (1-based).
            new_content (str): The new content to write.
            project_root (str): The root directory of the project.

        Returns:
            Dict[str, Any]: Result of the operation with status and message.
        """

        try:
            # Read the original file
            
            with open(file_path, "r", encoding="utf-8") as f:
                lines = f.readlines()

            # Validate line numbers (convert to 0-based indexing for Python)
            if start_line < 1 or end_line > len(lines) or start_line > end_line:
                return {"status": "error", "message": f"Invalid line range: start_line={start_line}, end_line={end_line}, file_lines={len(lines)}"}

            # Replace the chunk (start_line and end_line are 1-based, so adjust for 0-based indexing)
            start_idx = start_line - 1
            end_idx = end_line  # end_line is inclusive in the chunk, exclusive in slice
            # Split new_content into lines, ensuring it ends with a newline
            new_lines = new_content.strip().splitlines(keepends=False)
            new_lines = [line + "\n" for line in new_lines]
            # Adjust indentation to match the first line of the original chunk
            original_indent = len(lines[start_idx]) - len(lines[start_idx].lstrip())
            new_lines = [" " * original_indent + line.lstrip() for line in new_lines]
            # Replace the lines
            lines[start_idx:end_idx] = new_lines

            # Write the modified content back to the file
            with open(file_path, "w", encoding="utf-8") as f:
                f.writelines(lines)

            return {"status": "success", "message": f"Successfully overwrote chunk in {file_path} from line {start_line} to {end_line}"}

        except Exception as e:
            return {"status": "error", "message": f"Failed to overwrite chunk in {file_path}: {str(e)}"}

    def update_qdrant_after_file_change(self, file_path: str) -> Dict[str, Any]:
        """
        Updates the Qdrant database with new chunks after a file has been modified.

        Args:
            file_path (str): The relative path to the modified file.

        Returns:
            Dict[str, Any]: Result of the update operation.
        """
        try:
            # Since LaravelProcessor works on directories, we'll need to trick it into processing a single file
            # We'll use the appropriate method based on the file type
            
            file_path_obj = Path(file_path)
            file_type = get_file_type(file_path)

            # Clear existing chunks in the processor to avoid appending duplicates
            self.php_processor.chunks = []

            # Parse the single file
            if file_type == "blade_template":
                self.php_processor._parse_blade_file(file_path_obj)
            elif file_type == "route":
                self.php_processor._parse_route_file(file_path_obj)
            else:
                self.php_processor._parse_php_file(file_path_obj, file_type)

            new_chunks = self.php_processor.chunks
            LOGGER.info(f"Generated {len(new_chunks)} new chunks for {file_path}")

            if not new_chunks:
                LOGGER.warning(f"No new chunks generated for {file_path}. Qdrant not updated.")
                return {"status": "warning", "message": f"No new chunks generated for {file_path}"}

            # Step 2: Generate new embeddings
            contents_to_embed = [chunk.content for chunk in new_chunks]
            embeddings = self.embedding_model.embed_chunks(contents_to_embed)

            if not embeddings or len(embeddings) != len(new_chunks):
                LOGGER.error(f"Embedding failed or mismatch. Expected {len(new_chunks)} embeddings, got {len(embeddings) if embeddings else 0}.")
                return {"status": "error", "message": "Failed to generate embeddings for new chunks"}

            # Step 3: Delete old chunks from Qdrant
            success = self.vector_store.delete_chunks_by_file_path(file_path)
            if not success:
                LOGGER.error(f"Failed to delete old chunks for {file_path} from Qdrant.")
                return {"status": "error", "message": f"Failed to delete old chunks for {file_path} from Qdrant"}

            # Step 4: Upsert new chunks to Qdrant
            payloads = [chunk.model_dump() for chunk in new_chunks]
            success = self.vector_store.save_embeddings(embeddings, payloads)
            if not success:
                LOGGER.error(f"Failed to upsert new chunks for {file_path} to Qdrant.")
                return {"status": "error", "message": f"Failed to upsert new chunks for {file_path} to Qdrant"}

            LOGGER.info(f"Successfully updated Qdrant with {len(new_chunks)} new chunks for {file_path}")
            return {"status": "success", "message": f"Successfully updated Qdrant with {len(new_chunks)} new chunks for {file_path}"}

        except Exception as e:
            LOGGER.error(f"Error updating Qdrant for file {file_path}: {e}")
            return {"status": "error", "message": f"Error updating Qdrant for file {file_path}: {str(e)}"}

    def suggest_and_apply_code_update_replace(self, query: str) -> Dict[str, Any]:
        self.current_project_path = '/root/code_assistant/temp_code_uploads/leave-management-laravel'
        if not self.current_project_path:
            return {"status": "error", "message": "No codebase has been uploaded yet. Please upload a zip file first."}

        # 1. Perform Semantic Search
        processed_query_for_search = query
        try:
            query_embedding_list = self.embedding_model.embed_chunks([processed_query_for_search])
        except Exception as e:
            return {"status": "error", "message": f"Error generating embedding for the query: {e}"}

        if not query_embedding_list:
            return {"status": "error", "message": "Could not generate embedding for the query."}

        query_embedding = query_embedding_list[0]
        LOGGER.info(f"Searching Qdrant for top chunks for update query: '{processed_query_for_search}'")
        similar_chunks_payloads = self.vector_store.search_similar_chunks(embedding=query_embedding, limit=8)

        if not similar_chunks_payloads:
            return {"status": "info", "message": "No relevant code chunks found for the query. Cannot suggest changes."}

        code_chunks_for_llm = [{
            "file_path": payload.get("file_path", "Unknown"),
            "content": payload.get("content", "No content available for this chunk."),
            "start_line": payload.get("start_line", 0),
            "end_line": payload.get("end_line", 0)
        } for payload in similar_chunks_payloads]

        # 3. Generate Batch Modified Code in a single Gemini call
        if self.llm_model:
            try:
                modified_codes = self.llm_model.generate_batch_modified_code(query, code_chunks_for_llm)
                LOGGER.info(f"Generated batch modified codes: {modified_codes}")
            except Exception as e:
                LOGGER.error(f"Error generating batch modified code: {e}")
                return {"status": "error", "message": f"Failed to generate modified code: {str(e)}"}
        else:
            modified_codes = {0: "LLM unavailable or no chunks selected"}

        # 4. Apply Modified Code to All Files
        overall_status = "success"
        messages = ["Code updates suggested and application attempted."]
        modified_files = set()

        for idx_str, new_content in modified_codes.items():
            try:
                idx = int(idx_str)
            except ValueError:
                LOGGER.error(f"Invalid index format: {idx_str}, skipping.")
                messages.append(f"  Index {idx_str}: error - Invalid index format.")
                overall_status = "error"
                continue

            LOGGER.info(f"Processing index {idx}, type: {type(idx)}, len(code_chunks_for_llm): {len(code_chunks_for_llm)}")
            if idx >= len(code_chunks_for_llm):
                LOGGER.warning(f"Index {idx} out of range, skipping.")
                messages.append(f"  Index {idx}: error - Index out of range.")
                overall_status = "error"
                continue
            
            LOGGER.info(f"New content for index {idx}: {new_content[:50]}... (type: {type(new_content)})")
            if new_content in ["Need the model chunk to validate fields.", "Error: Invalid JSON response", "Error generating modification", "No modification generated"]:
                LOGGER.warning(f"Invalid or missing modification for index {idx}: {new_content}")
                messages.append(f"  Index {idx}: error - {new_content}")
                overall_status = "error"
                continue

            target_chunk = code_chunks_for_llm[idx]
            file_path = target_chunk["file_path"]
            start_line = target_chunk["start_line"]
            end_line = target_chunk["end_line"]

            if start_line == 0 or end_line == 0:
                LOGGER.error(f"Chunk at index {idx} does not contain valid line information, skipping.")
                messages.append(f"  File {file_path}: error - Invalid line information.")
                overall_status = "error"
                continue

            try:
                apply_result = self.overwrite_chunk_in_file(
                    file_path=file_path,
                    start_line=start_line,
                    end_line=end_line,
                    new_content=new_content,
                    project_root=self.current_project_path
                )
                messages.append(f"  File {file_path}: {apply_result['status']} - {apply_result['message']}")
                if apply_result["status"] == "success":
                    modified_files.add(file_path)
                else:
                    overall_status = "error" if overall_status == "success" else overall_status
            except Exception as e:
                LOGGER.error(f"Error applying modified code to {file_path}: {e}")
                messages.append(f"  File {file_path}: error - Failed to apply modified code: {str(e)}")
                overall_status = "error"

        # 5. Update Qdrant for All Modified Files
        for file_path in modified_files:
            try:
                update_result = self.update_qdrant_after_file_change(file_path)
                messages.append(f"  Qdrant update for {file_path}: {update_result['status']} - {update_result['message']}")
                if update_result["status"] != "success":
                    overall_status = "warning" if overall_status == "success" else overall_status
            except Exception as e:
                LOGGER.error(f"Error updating Qdrant for {file_path}: {e}")
                messages.append(f"  Qdrant update for {file_path}: error - Failed to update: {str(e)}")
                overall_status = "error"

        return {
            "status": overall_status,
            "message": "\n".join(messages)
        }    

# approach 2 by diff file

    # restored the original approach function
    # new approach for concatenated chunks
    def suggest_and_apply_code_update_new(self, query: str) -> Dict[str, Any]:
        """
        PoC method to suggest and apply code changes via diff.
        Skips LLM analysis type check and directly performs search -> diff generation -> diff application.
        """
        self.current_project_path='/root/code_assistant'
        # self.current_project_path='D:/codes/langGraph_venv/code_assistant'
        if not self.current_project_path:
            return {"status": "error", "message": "No codebase has been uploaded yet. Please upload a zip file first."}

        # 1. Perform Semantic Search
        processed_query_for_search = query
        try:
            query_embedding_list = self.embedding_model.embed_chunks([processed_query_for_search])
        except Exception as e:
            return {"status": "error", "message": f"Error generating embedding for the query: {e}"}

        if not query_embedding_list:
            return {"status": "error", "message": "Could not generate embedding for the query."}

        query_embedding = query_embedding_list[0]
        LOGGER.info(f"Searching Qdrant for top chunks for update query: '{processed_query_for_search}'")
        similar_chunks_payloads = self.vector_store.search_similar_chunks(embedding=query_embedding, limit=5) # Limit can be adjusted
        LOGGER.info(f"Found {len(similar_chunks_payloads)} similar chunks out of limit 5.")
        if not similar_chunks_payloads:
            return {"status": "info", "message": "No relevant code chunks found for the query. Cannot suggest changes."}

        # 2. Concatenate with adjacent chunks
        code_chunks_for_llm = self.vector_store.get_concatenated_adjacent_chunks(similar_chunks_payloads)
        if not code_chunks_for_llm:
            return {"status": "info", "message": "No concatenated chunks available for diff generation."}

        try:
            diff_output = self.llm_model.generate_code_diff(user_query=query, context_chunks=code_chunks_for_llm)
            # Basic check if LLM returned something that looks like a diff or an explanation
            if not diff_output or diff_output.strip().startswith("#") or not any(line.startswith(('--- ', '+++ ', '@@ ')) for line in diff_output.splitlines()):
                return {"status": "info", "message": "LLM did not generate a valid diff.", "diff": diff_output}
        except Exception as e:
            return {"status": "error", "message": f"Error generating diff with LLM: {e}"}

        try:
            diff_path = os.path.join(self.current_project_path, "temp_patch.diff")
            with open(diff_path, "w", newline="\n", encoding="utf-8") as f:
                f.write(diff_output)

            #rewrite repaired diff
            with open(diff_path, "w", newline="\n", encoding="utf-8") as f:
                f.write(diff_output)

            apply_results = self.apply_diff_with_unidiff(diff_output, self.current_project_path, diff_path)
            # apply_results = self.apply_diff_with_patch_lib(diff_output, self.current_project_path)
            # apply_results = self.apply_diff_with_patch_command(diff_output, self.current_project_path)
            # apply_results = self.apply_diff_with_git_apply(diff_output, self.current_project_path)
            
            # Check if any patch application failed or warned
            overall_status = "success"
            messages = ["Code changes suggested and application attempted."]
            for res in apply_results:
                messages.append(f"  File {res['file_path']}: {res['status']} - {res['message']}")
                if res['status'] == 'error':
                    overall_status = "error"
                elif res['status'] == 'warning' and overall_status != 'error':
                    overall_status = "warning"

            return {"status": overall_status, "message": "\n".join(messages), "diff": diff_output, "apply_results": apply_results}
        except Exception as e:
            apply_results = self.apply_diff_with_git_apply(diff_output, self.current_project_path)

            # This catch is for errors *before* the patch library starts processing files
            return {"status": "error", "message": f"An unexpected error occurred before applying diff: {e}", "diff": diff_output}
        # restored the original approach function
    def suggest_and_apply_code_update(self, query: str) -> Dict[str, Any]:
        """
        PoC method to suggest and apply code changes via diff.
        Skips LLM analysis type check and directly performs search -> diff generation -> diff application.
        """
        self.current_project_path='/root/code_assistant'
        # self.current_project_path='D:/codes/langGraph_venv/code_assistant'
        if not self.current_project_path:
            return {"status": "error", "message": "No codebase has been uploaded yet. Please upload a zip file first."}

        # 1. Perform Semantic Search
        processed_query_for_search = query # Skip analysis for PoC
        try:
            query_embedding_list = self.embedding_model.embed_chunks([processed_query_for_search])
        except Exception as e:
             return {"status": "error", "message": f"Error generating embedding for the query: {e}"}

        if not query_embedding_list:
            return {"status": "error", "message": "Could not generate embedding for the query."}

        query_embedding = query_embedding_list[0]
        LOGGER.info(f"Searching Qdrant for top chunks for update query: '{processed_query_for_search}'")
        similar_chunks_payloads = self.vector_store.search_similar_chunks(embedding=query_embedding, limit=5) 

    # original repair dff
    def suggest_and_apply_code_update(self, query: str) -> Dict[str, Any]:
        """
        PoC method to suggest and apply code changes via diff.
        Skips LLM analysis type check and directly performs search -> diff generation -> diff application.
        """
        self.current_project_path='/root/code_assistant/temp_code_uploads/leave-management-laravel'
        # self.current_project_path='D:/codes/langGraph_venv/code_assistant'

        if not self.current_project_path:
            return {"status": "error", "message": "No codebase has been uploaded yet. Please upload a zip file first."}

        # 1. Perform Semantic Search
        processed_query_for_search = query # Skip analysis for PoC
        try:
            query_embedding_list = self.embedding_model.embed_chunks([processed_query_for_search])
        except Exception as e:
             return {"status": "error", "message": f"Error generating embedding for the query: {e}"}

        if not query_embedding_list:
            return {"status": "error", "message": "Could not generate embedding for the query."}

        query_embedding = query_embedding_list[0]
        LOGGER.info(f"Searching Qdrant for top chunks for update query: '{processed_query_for_search}'")
        similar_chunks_payloads = self.vector_store.search_similar_chunks(embedding=query_embedding, limit=8) # Limit can be adjusted

        if not similar_chunks_payloads:
             return {"status": "info", "message": "No relevant code chunks found for the query. Cannot suggest changes."}

        code_chunks_for_llm = [{
            "file_path": payload.get("file_path", "Unknown"),
            "content": payload.get("content", "No content available for this chunk."),
            "start_line": payload.get("start_line"), # Ensure this is consistently available
            "end_line": payload.get("end_line"),     # Ensure this is consistently available
            "type": payload.get("type"),   
            "name": payload.get("name")
            } for payload in similar_chunks_payloads]
            
        try:
            diff_output = self.llm_model.generate_code_diff(user_query=query, context_chunks=code_chunks_for_llm)
            # Basic check if LLM returned something that looks like a diff or an explanation
            if not diff_output or diff_output.strip().startswith("#") or not any(line.startswith(('--- ', '+++ ', '@@ ')) for line in diff_output.splitlines()):
                 return {"status": "info", "message": "LLM did not generate a valid diff.", "diff": diff_output}

        except Exception as e:
            return {"status": "error", "message": f"Error generating diff with LLM: {e}"}

        # 3. Apply the diff
        try:
            diff_path = os.path.join(self.current_project_path, "temp_patch.diff")
            with open(diff_path, "w", newline="\n", encoding="utf-8") as f:
                f.write(diff_output)

            repaired_diff = self.repair_diff(diff_output)

            #rewrite repaired diff
            with open(diff_path, "w", newline="\n", encoding="utf-8") as f:
                f.write(repaired_diff)

            apply_results = self.apply_diff_with_unidiff(repaired_diff, self.current_project_path, diff_path)
            # apply_results = self.apply_diff_with_patch_lib(diff_output, self.current_project_path)
            # apply_results = self.apply_diff_with_patch_command(diff_output, self.current_project_path)
            # apply_results = self.apply_diff_with_git_apply(diff_output, self.current_project_path)
            
            # Check if any patch application failed or warned
            overall_status = "success"
            messages = ["Code changes suggested and application attempted."]
            for res in apply_results:
                 messages.append(f"  File {res['file_path']}: {res['status']} - {res['message']}")
                 if res['status'] == 'error':
                      overall_status = "error"
                 elif res['status'] == 'warning' and overall_status != 'error':
                      overall_status = "warning"

            return {"status": overall_status, "message": "\n".join(messages), "diff": diff_output, "apply_results": apply_results}

        except Exception as e:
            apply_results = self.apply_diff_with_git_apply(diff_output, self.current_project_path)

            # This catch is for errors *before* the patch library starts processing files
            return {"status": "error", "message": f"An unexpected error occurred before applying diff: {e}", "diff": diff_output}

    def apply_diff_with_patch_command(self, diff_content: str, project_root: str) -> List[Dict[str, Any]]:
        diff_path = os.path.join(project_root, "temp_patch.diff")
        with open(diff_path, "w", encoding="utf-8") as f:
            f.write(diff_content)
      
        try:
            result = subprocess.run(
                ["patch", "-p3", "-i", diff_path],
                cwd=project_root,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True
            )
            if result.returncode == 0:
                return [{"status": "success", "message": result.stdout}]
            else:
                return [{"status": "error", "message": result.stderr}]
        finally:
            os.remove(diff_path)

    def apply_diff_with_patch_lib(self, diff_content: str, project_root: str) -> List[Dict[str, Any]]:
        results = []
        try:
            patch_file_like = io.StringIO(diff_content)
            patch_file_like.seek(0)
            patch_set = PatchSet(patch_file_like)

            if not patch_set:
                results.append({"file_path": "N/A", "status": "error", "message": "Could not parse diff content. The diff might be empty or malformed."})
                return results

            for p_item in patch_set:
                try:
                    target_file_relative_to_diff = p_item.path
                    path_components = target_file_relative_to_diff.split(os.sep)
                    if len(path_components) < 2:
                        results.append({"file_path": p_item.path, "status": "error", "message": f"Invalid path format in diff: {p_item.path}. Expected 'a/path/to/file' or 'b/path/to/file'."})
                        continue

                    target_file_relative_to_root = os.sep.join(path_components[1:])
                    target_file_absolute_path = os.path.join(project_root, target_file_relative_to_root)

                    if not os.path.exists(target_file_absolute_path) and not p_item.is_added_file():
                        results.append({"file_path": p_item.path, "status": "error", "message": f"Target file for modification not found: {target_file_absolute_path}"})
                        continue

                    if p_item.is_added_file():
                        os.makedirs(os.path.dirname(target_file_absolute_path), exist_ok=True)

                    LOGGER.info(f"Attempting to apply patch to: {target_file_absolute_path}")
                    success = p_item.apply(strip=1, root=project_root)

                    if success is True:
                        results.append({"file_path": p_item.path, "status": "success", "message": "Patch applied successfully."})
                    elif success is False:
                        results.append({"file_path": p_item.path, "status": "warning", "message": "Patch application failed or applied with fuzz (context lines might not have matched exactly)."})
                    else:
                        results.append({"file_path": p_item.path, "status": "warning", "message": f"Patch application returned unexpected result: {success}"})
                except Exception as e_apply:
                    results.append({"file_path": p_item.path, "status": "error", "message": f"An unexpected error occurred during patch application for {p_item.path}: {e_apply}"})
        except Exception as e_parse:
            results.append({"file_path": "N/A", "status": "error", "message": f"Error parsing or processing diff content: {e_parse}"})
        return results
    
    def apply_diff_with_unidiff(self, diff_content: str, project_root: str, diff_path: str) -> List[Dict[str, Any]]:
        """
        Applies a unified diff string to files within the project root using the 'unidiff' library.
        Returns a list of results indicating success or failure for each file.
        """
        results = []
        try:
            with open(diff_path, "r", encoding="utf-8") as f:
                patch_set = PatchSet(f)
                LOGGER.info("Parsed OK", patch_set)
            for patched_file in patch_set:
                target_path =  patched_file.path

                if patched_file.is_removed_file:
                    if os.path.exists(target_path):
                        os.remove(target_path)
                        results.append({"file_path": target_path, "status": "success", "message": "File removed."})
                    else:
                        results.append({"file_path": target_path, "status": "warning", "message": "File marked for removal not found."})
                    continue

                if not patched_file.is_added_file and not os.path.exists(target_path):
                    results.append({"file_path": target_path, "status": "error", "message": "Original file does not exist."})
                    continue

                if patched_file.is_added_file:
                    os.makedirs(os.path.dirname(target_path), exist_ok=True)
                    original_lines = []
                else:
                    with open(target_path, "r", encoding="utf-8") as f:
                        original_lines = f.readlines()

                patched_lines = original_lines.copy()
                line_offset = 0

                for hunk in patched_file:
                    hunk_start = hunk.source_start - 1 + line_offset
                    expected_lines = [line.value for line in hunk if line.is_context or line.is_removed]
                    actual_lines = patched_lines[hunk_start:hunk_start + len(expected_lines)]

                    
                       
                    
                    def is_similar(a: str, b: str, threshold: float = 0.95) -> bool:
                        return SequenceMatcher(None, normalize_line(a), normalize_line(b)).ratio() >= threshold
                    
                    matched = all(
                        is_similar(exp, act)
                        for exp, act in zip(expected_lines, actual_lines)
                    )


                    #THIS FOR DEBUGGING
                    # for old_line, patch_line in zip([line for line in hunk if line.is_context or line.is_removed], actual_lines):
                    #     print(f"LINE: {normalize_line(old_line.value)}\nline: {normalize_line(patch_line)}")
                    #     if normalize_line(old_line.value) != normalize_line(patch_line):
                    #         break

                        
                    if not matched:
                        results.append({"file_path": target_path, "status": "error", "message": f"Hunk did not match context lines at line {hunk_start + 1}"})
                        break

                    hunk_len = len(expected_lines) or hunk.source_length
                    new_lines = [line.value for line in hunk if line.is_context or line.is_added]
                    patched_lines[hunk_start:hunk_start + hunk_len] = new_lines
                    line_offset += len(new_lines) - hunk_len

                else:
                    with open(target_path, "w", encoding="utf-8") as f:
                        f.writelines(patched_lines)
                    results.append({"file_path": target_path, "status": "success", "message": "Patch applied."})

        except Exception as e:
            results.append({"file_path": "N/A", "status": "error", "message": str(e)})
        finally:
            if os.path.exists(diff_path):
                os.remove(diff_path)

        return results
    
    def apply_diff_with_git_apply(self, diff_content: str, project_root: str) -> List[Dict[str, Any]]:
        """
        Applies a unified diff string using the 'git apply' command.
        This assumes the project_root is a git repository.
        """
        diff_path = os.path.join(project_root, "temp_patch.diff")
        with open(diff_path, "w", encoding="utf-8") as f:
            f.write(diff_content)

        try:
            # Check if the project_root is a git repository
            git_check_result = subprocess.run(
                ["git", "rev-parse", "--is-inside-work-tree"],
                cwd=project_root,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True
            )
            if git_check_result.returncode != 0 or git_check_result.stdout.strip() != "true":
                return [{"status": "error", "message": f"Project root '{project_root}' is not a git repository."}]

            # Run git apply
            result = subprocess.run(
                ["git", "apply", "--verbose", diff_path], # --verbose gives more output
                cwd=project_root,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True,
                check=False # Don't raise an exception on non-zero exit codes
            )

            if result.returncode == 0:
                return [{"status": "success", "message": f"Git apply successful.\nSTDOUT:\n{result.stdout}\nSTDERR:\n{result.stderr}"}]
            else:
                # git apply can fail for various reasons (e.g., patch does not apply)
                error_message = f"Git apply failed with return code {result.returncode}.\nSTDERR:\n{result.stderr}\nSTDOUT:\n{result.stdout}"
                return [{"status": "error", "message": error_message}]
        except FileNotFoundError:
            return [{"status": "error", "message": "The 'git' command was not found. Please ensure Git is installed and in your system's PATH."}]
        except Exception as e:
            return [{"status": "error", "message": f"An unexpected error occurred: {e}"}]
        finally:
            if os.path.exists(diff_path):
                os.remove(diff_path)


    def realign_hunk_to_matching_chunk(self, hunk: Hunk, chunk_metadata: dict[str, Any]) -> dict[str, Any]:
        """
        Tries to find the best matching chunk from metadata for a given set of hunk lines.
        """
        max_score = 0
        best_match = None

        hunk_text = "\n".join(normalize_line(line.value) for line in hunk if line.line_type in ('-', '+', ' '))

        for chunk in chunk_metadata.values():
            chunk_text = normalize_line(chunk.get("content", ""))
            score = SequenceMatcher(None, hunk_text, chunk_text).ratio()
            if score > max_score:
                max_score = score
                best_match = chunk

        return best_match if max_score > 0.5 else None  # Tune threshold as needed
    
    def realign_hunk_to_original_file(self, hunk: Hunk, original_lines: list[str]) -> tuple[str, bool]:
        """
        Tries to find the best matching location in the original file for the hunk's context lines.
        Returns the new starting line number and a boolean indicating success.
        """
        context_lines = [normalize_line(line.value) for line in hunk if line.line_type == ' ']
        if not context_lines:
            return hunk.source_start, False  # Nothing to match on

        for i in range(len(original_lines) - len(context_lines) + 1):
            match = True
            for j, context_line in enumerate(context_lines):
                if normalize_line(original_lines[i + j]) != context_line:
                    match = False
                    break
            if match:
                return i + 1, True  # Line numbers in diffs are 1-based

        return hunk.source_start, False  # No match found

    def repair_diff(self, diff_content: str, chunk_metadata: dict[str, Any] = {}) -> str:
        """
        Repairs malformed hunk headers in a unified diff string by aligning with chunk metadata.
        """
        patch_set = PatchSet(io.StringIO(diff_content))
        # with open(diff_path, "r", encoding="utf-8") as f:
        #     cpatch_set = PatchSet(f)

        for patched_file in patch_set:
            for hunk in patched_file:
                # best_chunk = self.realign_hunk_to_matching_chunk(list(hunk), chunk_metadata)
                # if not best_chunk:
                #     continue  # Skip if no matching chunk

                # new_start = best_chunk["start_line"]

                # Split hunk lines
                old_lines = [l for l in hunk if l.is_removed or l.is_context]
                new_lines = [l for l in hunk if l.is_added or l.is_context]

                # Repair header
                # hunk.source_start = new_start
                hunk.source_length = len(old_lines)
                # hunk.target_start = new_start
                hunk.target_length = len(new_lines)

        return str(patch_set)

    def validate_and_adjust_diff_lines(self, diff_content: str, file_path: str, project_root: str) -> str:
        """
        Validates and adjusts diff line numbers by matching normalized context.
        """
        try:
                # Read the target file as lines
            target_file_path = os.path.join(project_root, file_path)
            with open(target_file_path, "r", encoding="utf-8") as f:
                file_lines = [line for line in f.readlines()]  # Read as lines first
            file_lines = [normalize_line(line) for line in file_lines]  # Normalize each line

            adjusted_diff_lines = []
            current_file_line = 0
            in_hunk = False
            hunk_header = ""
            for line in diff_content.splitlines():
                if line.startswith("@@"):
                    if in_hunk:
                        adjusted_diff_lines.append(hunk_header)  # Add the last adjusted header
                    hunk_header = line
                    in_hunk = True
                    current_file_line = 0  # Reset for new hunk
                    adjusted_diff_lines.append(line)
                    continue
                elif line.startswith(("---", "+++", "diff --git")):
                    adjusted_diff_lines.append(line)
                    continue
                elif not in_hunk:
                    adjusted_diff_lines.append(line)
                    continue

                # Process hunk lines
                if in_hunk:
                    if line.startswith(' '):  # Context line
                        if current_file_line < len(file_lines):
                            file_line = file_lines[current_file_line]  # Already normalized string
                            diff_line = normalize_lines(line)  # Normalize diff context line
                            if file_line == diff_line:  # Match after normalization
                                current_file_line += 1
                                adjusted_diff_lines.append(line)  # Preserve original line with indentation
                            else:
                                # Search for the next match with normalization, limit to avoid infinite loops
                                search_limit = min(100, len(file_lines) - current_file_line)  # Cap search to 100 lines
                                for i in range(current_file_line, current_file_line + search_limit):
                                    if file_lines[i] == diff_line:
                                        current_file_line = i + 1
                                        adjusted_diff_lines.append(line)  # Preserve original line
                                        break
                                else:
                                    LOGGER.warning(f"No match for context line '{line[1:].rstrip()}' near line {current_file_line}. Skipping.")
                    elif line.startswith('-'):  # Removed line
                        current_file_line += 1
                    elif line.startswith('+'):  # Added line
                        adjusted_diff_lines.append(line)  # Preserve original indentation

                    # Update header if we’ve processed some lines
                    if current_file_line > 0 and ' ' in [l[0] for l in adjusted_diff_lines[-5:]]:
                        match = re.match(r"@@ -(\d+),(\d+) \+(\d+),(\d+) @@", hunk_header)
                        if match:
                            new_source_start = current_file_line - len([l for l in adjusted_diff_lines[-5:] if l.startswith(' ')]) + 1
                            new_source_length = len([l for l in adjusted_diff_lines[-5:] if l.startswith((' ', '-'))])
                            new_target_length = len([l for l in adjusted_diff_lines[-5:] if l.startswith((' ', '+'))])
                            hunk_header = hunk_header.replace(
                                f"-{match.group(1)},{match.group(2)} +{match.group(3)},{match.group(4)}",
                                f"-{new_source_start},{new_source_length} +{new_source_start},{new_target_length}"
                            )

            # Add the last header if in hunk
            if in_hunk:
                adjusted_diff_lines.append(hunk_header)

            return "\n".join(adjusted_diff_lines)

        except Exception as e:
            LOGGER.error(f"Error validating and adjusting diff lines: {e}")
            return diff_content