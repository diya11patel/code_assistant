import shutil
import zipfile
import os
from typing import Dict, List
from fastapi import HTTPException, UploadFile # Keep HTTPException if used elsewhere, UploadFile is used

from db_connection.qdrant import QdrantDBManager, COLLECTION_NAME 
from dto.value_objects import ChunkResponse, CodeChunk, QueryResponse, UserQueryAnalysisType, LLMQueryResponse, CodeReference
from langugae_processors.php_processor import LaravelProcessor 
from model_interfaces.embedding_model import EmbeddingModel
from model_interfaces.gemini_model import GeminiModel



class ChatAssistantService():
    """
    Service class for handling chat assistant operations.
    Service class for handling chat assistant operations.
    This will encapsulate the logic for uploading, processing,
    and querying codebases.
    """
    def __init__(self):
        self.embedding_model = EmbeddingModel()
        self.vector_store = QdrantDBManager()
        try:
            self.llm_model = GeminiModel()
        except Exception as e: 
            print(f"Failed to initialize Gemini Model: {e}. LLM features will be unavailable.")
            self.llm_model = None
        pass

    async def process_uploaded_zip(self, zip_file: UploadFile, description: str) -> list[str]:
        """
        Handles the end-to-end process of receiving a zip file,
        saving, extracting, and initiating processing (parsing, embedding, storage).

        Args:
            zip_file (UploadFile): The uploaded zip file.
            description (str): Description of the codebase.

        Returns:
            list[str]: A list of file paths that were extracted.

        """
        print("Started codebase Processing")
        # project_path = r"D:/codes/langGraph_venv/code_assistant/temp_code_uploads/leave-management-laravel"
        project_path =  self.save_uploaded_zip(zip_file, description)
        result = await self.process_codebase(project_path, description)
        return True

    
    

    async def process_codebase(self, project_path: str, description: str) -> List[ChunkResponse]: # Adjusted return type for clarity, though caller ignores it
        # self.lang_processor = LaravelProcessor(project_path)
        # chunks = self.lang_processor.chunk_codebase()

        self.php_processor = LaravelProcessor()
        code_chunks: List[CodeChunk] = self.php_processor.analyze_codebase(project_path)
        
        if not code_chunks:
            print("No code chunks were generated by the processor.")
            return [] # Return empty list if no chunks

        # Extract the actual code content for embedding
        contents_to_embed = [chunk.content for chunk in code_chunks]
        embeddings = self.embedding_model.embed_chunks(contents_to_embed)

        if not embeddings or len(embeddings) != len(code_chunks):
            print(f"Embedding failed or mismatch. Expected {len(code_chunks)} embeddings, got {len(embeddings) if embeddings else 0}.")
            return [] # Return empty list on embedding failure

        # Use the full chunk data (as dictionaries) for payloads in Qdrant
        payloads = [chunk.model_dump() for chunk in code_chunks] # Use .model_dump() for Pydantic models
        success = self.vector_store.save_embeddings(embeddings, payloads)

        if not success:
            print("Failed to save embeddings to vector store.")
            return [] # Return empty list on storage failure
        return code_chunks # Return the processed chunks

                            
    def save_uploaded_zip(self, file: UploadFile, description: str) -> str:
        """
        Saves the uploaded zip file, extracts its contents.
        Returns a list of full paths to the extracted files.
        """
        #create temp folder th root level of the project
        TEMP_DIR = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "temp_code_uploads")
        # Ensure TEMP_DIR exists (it should be created by main.py on startup)
        if not os.path.exists(TEMP_DIR):
            os.makedirs(TEMP_DIR, exist_ok=True)
            print(f"Warning: TEMP_DIR '{TEMP_DIR}' was created by api_router.py. It should ideally be managed by main.py startup.")


        extracted_file_paths = []
        temp_zip_path = os.path.join(TEMP_DIR, file.filename)
        # Ensure extraction path is unique if multiple zips with same name are uploaded (though current logic overwrites)
        extraction_base_name = os.path.splitext(file.filename)[0]
        extraction_path = os.path.join(TEMP_DIR, extraction_base_name)

        try:
            # Save the uploaded zip file
            with open(temp_zip_path, "wb") as buffer:
                shutil.copyfileobj(file.file, buffer)

            # Create extraction directory, removing if it already exists to ensure clean extraction
            if os.path.exists(extraction_path):
                shutil.rmtree(extraction_path)
            os.makedirs(extraction_path, exist_ok=True)

            with zipfile.ZipFile(temp_zip_path, 'r') as zip_ref:
                zip_ref.extractall(extraction_path)
                # Get list of extracted files (full paths)
                for item_name in zip_ref.namelist():
                    full_item_path = os.path.join(extraction_path, item_name)
                    extracted_file_paths.append(full_item_path)

            print(f"Received description for {file.filename}: {description}")
            print(f"Extracted files for {file.filename} to: {extraction_path}")

            # Clean up the temporary zip file after extraction
            os.remove(temp_zip_path)

        except Exception as e:
            # Clean up in case of error
            if os.path.exists(temp_zip_path):
                os.remove(temp_zip_path)
            # If extraction path was created but extraction failed or was partial, clean it up
            if os.path.exists(extraction_path) and not any(os.path.exists(p) for p in extracted_file_paths):
                shutil.rmtree(extraction_path)
            raise HTTPException(status_code=500, detail=f"Failed to process zip file '{file.filename}': {str(e)}")
        finally:
            # Ensure the file object is closed
            if hasattr(file, 'file') and file.file:
                file.file.close()

        return extraction_path

    def query_for_semantic_search(self, query: str, top_k: int = 5) -> QueryResponse:
        """
        Embeds the user query and searches for similar code chunks in Qdrant.

        Args:
            query (str): The user's question/query string.
            top_k (int): The number of top similar chunks to retrieve.

        Returns:
            QueryResponse: An object containing the original question, a list of
                           relevant chunk responses, and the count of found chunks.
        """
        original_user_query = query # Store the original query

        if not original_user_query:
            return QueryResponse(
                question=original_user_query, 
                llm_answer=LLMQueryResponse(explanation="Query was empty.", code_references=[]), 
                retrieved_chunks=[], 
                relevant_chunks_found=0)

        processed_query_for_search = original_user_query

        if self.llm_model:
            print(f"Analyzing query with LLM: '{original_user_query}'")
            try:
                # GeminiModel.analyze_query now returns a parsed UnifiedQueryAnalysis object
                analysis_result = self.llm_model.analyze_query(original_user_query)
                print(f"LLM analysis result: type='{analysis_result.type.value}', response='{analysis_result.response}'")

                if analysis_result.type == UserQueryAnalysisType.GENERAL_ANSWER:
                    print(f"LLM provided direct answer: {analysis_result.response}")
                    return QueryResponse(
                        question=original_user_query,
                        llm_answer=LLMQueryResponse(explanation=analysis_result.response, code_references=[]),
                        retrieved_chunks=[],
                        relevant_chunks_found=0
                    )
                elif analysis_result.type == UserQueryAnalysisType.REFINED_QUERY:
                    if analysis_result.response and analysis_result.response.strip():
                        processed_query_for_search = analysis_result.response
                        print(f"LLM refined query to: '{processed_query_for_search}'")
                    else:
                        print("LLM returned an empty refined query, using original.")
                else:
                    print(f"LLM analysis returned unknown type: '{analysis_result.type.value}'. Proceeding with original query.")

            except Exception as e:
                print(f"Error during LLM query analysis: {e}. Proceeding with original query.")
        else:
            print("LLM model not available for query pre-processing.")

        # --- Semantic Search with processed_query_for_search ---
        print(f"Embedding processed query for search: '{processed_query_for_search}'")
        query_embedding_list = self.embedding_model.embed_chunks([processed_query_for_search])

        if not query_embedding_list:
            print("Could not generate embedding for the query.")
            return QueryResponse(
                question=original_user_query, 
                llm_answer=LLMQueryResponse(explanation="Could not generate embedding for the query.", code_references=[]), 
                relevant_chunks_found=0, 
                retrieved_chunks=[])
        
        query_embedding = query_embedding_list[0]

        print(f"Searching Qdrant for top {top_k} similar chunks based on: '{processed_query_for_search}'")
        similar_chunks_payloads = self.vector_store.search_similar_chunks(embedding=query_embedding, limit=top_k)
        
        retrieved_chunk_responses: List[ChunkResponse] = []
        code_chunks_for_llm: List[Dict[str, str]] = []

        if similar_chunks_payloads:
            for payload in similar_chunks_payloads:
                file_name = payload.get("file_path", "Unknown")
                chunk_content = payload.get("content", "No content available for this chunk.")
                
                retrieved_chunk_responses.append(
                    ChunkResponse(file_name=file_name, chunk=chunk_content)
                )
                code_chunks_for_llm.append({
                    "file_path": file_name,
                    "content": chunk_content
                })
        
        # --- Generate Final Answer using LLM with context (if any) ---
        final_llm_answer: LLMQueryResponse # Type hint
        if self.llm_model:
            if code_chunks_for_llm: # If chunks were found
                print(f"Sending original query {original_user_query} and {len(code_chunks_for_llm)} chunks to LLM for final answer...")
                final_llm_answer = self.llm_model.generate_response(user_query=original_user_query, context_chunks=code_chunks_for_llm)
            else: # No relevant chunks found after search
                print(f"No relevant chunks found. Sending original query {original_user_query} to LLM without specific context chunks.")
                final_llm_answer = self.llm_model.generate_response(user_query=original_user_query, context_chunks=[])
        elif not self.llm_model: # LLM model was never available
            final_llm_answer = LLMQueryResponse(
                explanation="LLM model is not available to generate an answer.",
                code_references=[]
            )
        else: # Should not be reached if logic is correct, but as a fallback
            final_llm_answer = LLMQueryResponse(
                explanation="Could not generate an answer using the LLM due to an unexpected state.",
                code_references=[]
            )
        

        return QueryResponse(
            question=original_user_query, # Always return the original question
            llm_answer=final_llm_answer,
            retrieved_chunks=retrieved_chunk_responses,
            relevant_chunks_found=len(retrieved_chunk_responses)
        )