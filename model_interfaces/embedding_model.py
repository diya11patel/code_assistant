from sentence_transformers import SentenceTransformer
from typing import List
import logging
from utils.logger import LOGGER

import torch


class EmbeddingModel:
    """
    A wrapper class for handling sentence embedding models.
    This class initializes a pre-trained model and provides
    methods to generate embeddings for text.
    """
    def __init__(self):
        """
        Initializes the EmbeddingModel.

        Args:
            model_name (str): The name of the pre-trained sentence transformer model to use.
        """
        self.model_name = "BAAI/bge-code-v1"
        self.model = None
        self._load_model()
 
    def _load_model(self):
        """
        Loads the pre-trained sentence transformer model.
        Handles potential errors during model loading.
        """
        try:
            LOGGER.info(f"Loading embedding model: {self.model_name}...")
            self.model = SentenceTransformer(
                        self.model_name,
                        trust_remote_code=True,
                        model_kwargs={"torch_dtype": torch.float16},
                    ) 
            LOGGER.info(f"Embedding model '{self.model_name}' loaded successfully.")
        except Exception as e:
            LOGGER.error(f"Failed to load embedding model '{self.model_name}': {e}")
            # Depending on the application's needs, you might want to re-raise the exception
            # or handle it in a way that allows the application to continue with limited functionality.
            raise RuntimeError(f"Could not initialize embedding model: {e}")

    def embed_chunks(self, text_chunks: List[str], batch_size: int = 10) -> List[List[float]]:
        """
        Generates embeddings for a list of text chunks.

        Args:
            text_chunks (List[str]): A list of text strings to embed.
            batch_size (int): The number of chunks to process in each batch.

        Returns:
            List[List[float]]: A list of embeddings, where each embedding is a list of floats.
                               Returns an empty list if the input is empty or the model is not loaded.
        """
        if not self.model:
            LOGGER.error("Embedding model is not loaded. Cannot generate embeddings.")
            return []

        if not text_chunks:
            LOGGER.info("No text chunks provided for embedding.")
            return []

        try:
            all_embeddings = []
            LOGGER.info(f"Generating embeddings for {len(text_chunks)} chunks in batches of {batch_size}...")
            
            for i in range(0, len(text_chunks), batch_size):
                batch = text_chunks[i:i + batch_size]
                LOGGER.info(f"Processing batch {i // batch_size + 1}/{(len(text_chunks) + batch_size - 1) // batch_size}...")
                batch_embeddings_np = self.model.encode(batch, show_progress_bar=True) # Progress bar per batch might be too verbose
                
                # Convert numpy arrays to lists of floats for each embedding in the batch
                batch_embeddings_list = [embedding.tolist() for embedding in batch_embeddings_np]
                all_embeddings.extend(batch_embeddings_list)

            LOGGER.info("Embeddings generated successfully.")
            return all_embeddings
        except Exception as e:
            LOGGER.error(f"Error during embedding generation: {e}")
            return [] # Or re-raise, depending on desired error handling

    def get_embedding_dimension(self) -> int:
        """
        Returns the dimension of the embeddings generated by the model.

        Returns:
            int: The embedding dimension, or -1 if the model is not loaded.
        """
        if not self.model:
            LOGGER.warning("Embedding model is not loaded. Cannot determine embedding dimension.")
            return -1
        return self.model.get_sentence_embedding_dimension()
